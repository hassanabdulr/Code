---
title: "SPIN-R"
output: html_document
date: "`r Sys.Date()`"
---

# Libraries 

```{r}
library(tidyr)
library(dplyr)
library(ggplot2)
library(lubridate)

```


# Data Preparation And Wrangling 

Load up data

```{r}

Data <- read.csv("/home/aabdulrasul/Documents/Projects/SPINR/SPN30IdentificationO_DATA_2024-09-05_1113.csv")

# Run any pre-processing steps here that make the data readable - e.g, turn blanks "" into NA

# Replace blank with NA

replace_blank_with_na <- function(x) {
  if (is.character(x)) {
    x <- na_if(x, "")
  }
  return(x)
}

Data <- Data %>% mutate(across(everything(), ~ replace_blank_with_na(.)))
  
```

### Merge

Data set has 4 arms, we will separate them into 4 separate data frames. The logic behind this is to clean and process each on before we merge them into one final data frame

```{r}

Arm1 <- Data %>% filter(grepl("_arm_1$", redcap_event_name)) # ASD
Arm2 <- Data %>% filter(grepl("_arm_2$", redcap_event_name)) # SSD in SPINS
Arm3 <- Data %>% filter(grepl("_arm_3$", redcap_event_name)) # Control
Arm4 <- Data %>% filter(grepl("_arm_4$", redcap_event_name)) # Repeat scan using some of the original study 
```

Filter out time_points in Arm4

```{r}

# Specifically in Arm4 we have instances where record_ids have separate rows for their respective time points. We want to ensure the merging process does not treat these instances as one. As such we are going to filter out these instances and save them in a new data frame _tx2 

Arm4_tx2 <- Arm4 %>% 
  filter(redcap_event_name == "tx2_arm_4")

Arm4 <- Arm4 %>%
  filter(redcap_event_name != "tx2_arm_4")

# For some reason Arm4 has a record_id named test - we will filter this out too

Arm4 <- Arm4 %>%
  filter(record_id != "test")
```

### Merge : Find Duplicates

Compare record_ids between the arms to determine if there are any duplicates.

```{r}

# First, extract the record_id columns from each dataframe
record_ids_arm_1 <- Arm1$record_id
record_ids_arm_2 <- Arm2$record_id
record_ids_arm_3 <- Arm3$record_id
record_ids_arm_4 <- Arm4$record_id

# Find common record_ids between each pair of arms
common_arm_1_2 <- intersect(record_ids_arm_1, record_ids_arm_2)
common_arm_1_3 <- intersect(record_ids_arm_1, record_ids_arm_3)
common_arm_1_4 <- intersect(record_ids_arm_1, record_ids_arm_4)
common_arm_2_3 <- intersect(record_ids_arm_2, record_ids_arm_3)
common_arm_2_4 <- intersect(record_ids_arm_2, record_ids_arm_4)
common_arm_3_4 <- intersect(record_ids_arm_3, record_ids_arm_4)

# Print the results
cat("Common IDs between Arm 1 and Arm 2:", common_arm_1_2, "\n")
cat("Common IDs between Arm 1 and Arm 3:", common_arm_1_3, "\n")
cat("Common IDs between Arm 1 and Arm 4:", common_arm_1_4, "\n")
cat("Common IDs between Arm 2 and Arm 3:", common_arm_2_3, "\n")
cat("Common IDs between Arm 2 and Arm 4:", common_arm_2_4, "\n")
cat("Common IDs between Arm 3 and Arm 4:", common_arm_3_4, "\n")
```


Filter out problematic duplicate record id : SPN30_CMH_043281 exists in both Arm2 and Arm4, however, Arm2 has a partially completed timepoint 1 dataset, this is causing some issues in the merge process as the same participant seemingly has completed the timepoint 1 battery in Arm4 which was completed a couple of days apart from Arm2. As such we can assume Arm4 is the more accurate dataset. therefore we will remove SPN30_CMH_043281 from Arm2.


```{r}
Arm2 <- Arm2 %>%
  filter(record_id != "SPN30_CMH_043281")
```


### Merge : Standardize duplicates

Now that we've determined Arm 2 and Arm 4 both have duplicates, we need to ensure that the information in these data frames are in fact duplicates of each other - or whether one has more data points filled than the other. We will essentially compare the 2 and then copy whatever is extra onto Arm 4 (as we are currently working with this data set).

Note that this is hard coded for Arm4 and Arm2, update and adapt where applicable based on the results of the previous cell


```{r}

# Identify common record_ids across Arm2 and 4
common_arm_2_4 <- intersect(Arm2$record_id, Arm4$record_id) 

# Filter both data frames to include only the common record IDs
filtered_Arm2 <- Arm2 %>% filter(record_id %in% common_arm_2_4)
filtered_Arm4 <- Arm4 %>% filter(record_id %in% common_arm_2_4)

# Merge the filtered data from Arm2 and 4 based on record_id
comparison_df <- merge(filtered_Arm2, filtered_Arm4, by = "record_id", suffixes = c("_arm2", "_arm4"))

# Compare each column and update directly
for (col in setdiff(names(filtered_Arm2), "record_id")) {
  col_arm2 <- paste0(col, "_arm2")
  col_arm4 <- paste0(col, "_arm4")
  
  # Compare and decide to update
  comparison_df[[col_arm4]] <- ifelse(comparison_df[[col_arm2]] != comparison_df[[col_arm4]] & !is.na(comparison_df[[col_arm2]]),
                                      comparison_df[[col_arm2]],
                                      comparison_df[[col_arm4]])
}

# Drop the arm2 columns and rename arm4 columns to their original names
updated_arm_4 <- comparison_df %>%
  select(record_id, ends_with("_arm4")) %>%
  rename_with(~ sub("_arm4", "", .), ends_with("_arm4"))

# Append these rows to updated_arm_4
complete_arm_4 <- bind_rows(updated_arm_4,  anti_join(Arm4, updated_arm_4, by = "record_id"))

```


Now that we have the completed arms, we will now designate them into their respective groups

Arm1 corresponds to ASD
Arm2 corresponds to SSD/SPINS
Arm3 corresponds to Control
Arm4 corresponds to SPINR (not this will include the duplicates seen in Arm2)

```{r}
Arm1 <- Arm1 %>%
  mutate(redcap_event_name = "ASD") %>%
  rename(group = redcap_event_name)

Arm2 <- Arm2 %>%
  mutate(redcap_event_name = "SSD") %>%
  rename(group = redcap_event_name)

Arm3 <- Arm3 %>%
  mutate(redcap_event_name = "Control") %>%
  rename(group = redcap_event_name)

complete_arm_4 <- complete_arm_4 %>%
  mutate(redcap_event_name = "SPINR") %>%
  rename(group = redcap_event_name)
```

(Optional) Remove intermediary data frames

```{r}
rm(comparison_df, filtered_Arm2, filtered_Arm4, updated_arm_4)
```


Now that we have the duplicates sorted we can now merge the arms together, note that we are going to merge Arm1, Arm2, Arm3 and Arm4. The steps above have effectively combined the duplicates seen in Arm2 and Arm4 under complete_arm_4 which means that when we merge the arms together, we need to ensure we are ONLY merging the non-duplicates from Arm2. All Arms will be merged as the dataframe MergedDF 

```{r}

# Merge all arms together into one dataframe. Anti_join is used here to find the unique record_ids between Arm2 and complete_arm_4 thereby allowing us to merge the non duplicates in Arm2, 

MergedDF <- bind_rows(Arm1, anti_join(Arm2, complete_arm_4, by = "record_id"), Arm3, complete_arm_4)

# We will now collapse repeated record_ids to form one record_id filling in 

# Collapse duplicate record_id entries in MergedD` by grouping by record_id and summarizing each column for each column within the same `record_id`. Using ~na.omit we remove all NA values and retain only the first non-NA value encountered, therefore it doesn't matter whether the non-NA value is in the first or second row. This gives us a single consolidated row per record_id if a duplicate exists (there are no triple repeats). If all values are NA within a column for a specific record_id, the resulting value for that column will be NA.

MergedDF <- MergedDF %>%
  group_by(record_id) %>%
  summarise_all(~na.omit(.)[1])

```



## Demographics

```{r}

# Create a new dataframe SPINR_demo which will house the main demographic information from MergedDF. This includes the participants, age at the start of the study, their handedness, group, assigned sex at birth, race, highest education amd neuropsych composite score

# This was adapted from SPASD_SPINS_redcap_wrangling.Rmd

SPINR_demo <-
  MergedDF %>%
  select(record_id, group, demo_doa,demo_dob, np_fact_handedness,
         demo_sex_birth, demo_age_study_entry,
         demo_race___1_asian_east, demo_race___2_asian_southeast, demo_race___3_asian_south,
         demo_race___4_black_african, demo_race___5_black_na, demo_race___6_black_carribean,
         demo_race___7_firstnations, demo_race___8_indian_caribbean, demo_race___9_indigenous, demo_race___10_inuit,            demo_race___11_latin, demo_race___12_metis, demo_race___13_middleeastern, demo_race___14_white,                        demo_race___15_white_na, demo_race___16_mixed, demo_race___17_noanswer, demo_race___18_unknown,                        demo_race___19_other, demo_highest_grade_self, np_composite_tscore) %>%

  # We are setting assigned sex at birth as a factor for analysis
  
    mutate(sex = factor(demo_sex_birth,
                      levels = c(1, 2, 3, 4),
                      labels = c("Female", "Male", "Intersex", "Prefer not to Answer")),
 
  # Renaming the responses for the demo race drop down to be more clear. case_when used to vectorise the data thereby giving us one column - race - with the respective participants responses.
  
            race = case_when(
                          demo_race___1_asian_east == 1 ~ "Asian - East",
                          demo_race___2_asian_southeast == 1 ~ "Asian - South East",
                          demo_race___3_asian_south == 1 ~ "Asian - South Asia",
                          demo_race___4_black_african == 1 ~ "Black - African",
                          demo_race___5_black_na == 1 ~ "Black - African American",
                          demo_race___6_black_carribean == 1 ~ "Black - Caribbean",
                          demo_race___7_firstnations == 1 ~ "Native - First Nation",
                          demo_race___8_indian_caribbean == 1 ~ "South Asian - Caribbean",
                          demo_race___9_indigenous == 1 ~ "Native - American",
                          demo_race___10_inuit == 1 ~ "Native - Inuit",
                          demo_race___11_latin == 1 ~ "Latin American",
                          demo_race___12_metis == 1 ~ "Native - Metis",
                          demo_race___13_middleeastern == 1 ~ "Middle Eastern",
                          demo_race___14_white == 1 ~ "White - European",
                          demo_race___15_white_na == 1 ~ "White - North American",
                          demo_race___16_mixed == 1 ~ "More than one race",
                          demo_race___17_noanswer == 1 ~ "No Answer",
                          demo_race___18_unknown == 1 ~ "Unknown",
                          demo_race___18_unknown == 1 ~ "Other" ))

# Construct final dataframe with the post processed columns

SPINR_demo <- SPINR_demo %>%
  select(record_id, group, demo_doa, demo_dob, np_fact_handedness, sex, demo_age_study_entry, race, demo_highest_grade_self, np_composite_tscore)
                          
        
```


## Neuropsych Assessments

The following Neuropsych assessments are collected, RMET (Reading The Mind in The Eyes Test), ER-40 (Penn Emotion Recognition Test), and, TASIT-R (The Awareness of Social Inference Test - Revised) Along with various clinical assessments. We will create a new dataframe combining all of these into one from MergedDF - this new dataframe will be called SPINR_neuro

Adapted from SPASD_SPINS_redcap_wrangling.Rmd

```{r}


SPINR_neuro <-
    MergedDF %>%
      select(record_id,
             
             # Brief Psychiatric Rating Scale
             
             bprs_factor_anxiety_depression,
             bprs_factor_neg_symp,
             bprs_factor_pos_symp,
             bprs_factor_activation,
             bprs_factor_hostility,
             
             # Neuropsych Summary Scores
             
             np_domain_tscore_process_speed,
             np_domain_tscore_att_vigilance,
             np_domain_tscore_work_mem,
             np_domain_tscore_verbal_learning,
             np_domain_tscore_visual_learning,
             np_domain_tscore_reasoning_ps,
             np_domain_tscore_social_cog,
             
             # Interpersonal Reactivity Index
             
             iri_factor_pt,
             iri_factor_fs,
             iri_factor_ec,
             iri_factor_pd,
             iri_total,
             
             # Birchwood Social Functioning Scale
             
             bsfs_sec1_total,
             bsfs_sec2_total,
             bsfs_sec3_total,
             bsfs_sec4_total,
             bsfs_sec5_total,
             bsfs_sec6_total,
             bsfs_sec7_y_total_7a,
             bsfs_sec7_n_total_7b,   
             bsfs_sec7_y_total_7a,
             bsfs_sec_grandtotal1,
             bsfs_sec_grandtotal2,
          
             # Penn Emotion Recognition Test
             
             er40_cr,
             er40_crt,
             er40ang,
             er40fear,
             er40hap,
             er40noe,
             er40sad,
             er40_fpa,
             er40_fpf,
             
             # Reading The Mind In The Eyes Test
           
             rmet_total,
             
             # The Awareness Of Social Inference Test-Revised (TASIT)
             
             tasit_part1_happyscore, 
             tasit_part2_surprisedscore,
             tasit_part3_neutralscore,
             tasit_part4_sadscore,
             tasit_part5_angryscore,
             tasit_part6_anxiousscore,
             tasit_part7_revoltedscore,
             
             tasit_positive_total,
             tasit_negative_total,
             tasit_correct_total,
             tasit_part2_grandtotal,
             tasit_part2_total_do_sincere,
             tasit_part2_total_say_sincere,
             tasit_part2_total_think_sincere,
             tasit_part2_total_feel_sincere,
             tasit_part2_total_do_s_sarcasm,
             tasit_part2_total_say_s_sarcasm,
             tasit_part2_total_think_s_sarcasm,
             tasit_part2_total_feel_s_sarcasm,
             tasit_part2_total_do_p_sarcasm,
             tasit_part2_total_say_p_sarcasm,
             tasit_part2_total_think_p_sarcasm,
             tasit_part2_total_feel_p_sarcasm,
             
             tasit_part2_total_sincere,
             tasit_part2_total_s_sarcasm,
             tasit_part2_total_p_sarcasm,
             
             # TAST - Total Number of items correct
             
             tasit_part2_total_do,
             tasit_part2_total_say,
             tasit_part2_total_think,
             tasit_part2_total_feel,
             
  
             # TASIT - Total Correct Sarcastic - Do, Say, Think, Feel
             
             tasit_part3_total_sarcastic_do,
             tasit_part3_total_sarcastic_say,
             tasit_part3_total_sarcastic_think,
             tasit_part3_total_sarcastic_feel,
             
             # TASIT - Total Correct - Text and Visual Cues
             
             tasit_part3_grandtotal_text,
             tasit_part3_grandtotal_visual,
             
             # TASIT - Total Correct - Do, Say, Think, Feel
             
             tasit_part3_grandtotal_do,
             tasit_part3_grandtotal_say,
             tasit_part3_grandtotal_think,
             tasit_part3_grandtotal_feel,
             
             # Schizotypal Personality Questionnaire-Brief
             
             spqb_total,
             
             # Adult Autism-Spectrum Quotient
             
             aq_total,
             
             # Beck Depression Inventory
             
             total_score)

# Rename Beck Depression Inventory - the variable is currently total_score, which is super ambiguious - renaming variable to bdi_total.

names(SPINR_neuro)[names(SPINR_neuro) == 'total_score'] <- 'bdi_total'

```


# Analysis


### Model

We are now going to construct a new data frame SPINR_model - that combines our variables of interest from the SPINR_demo and SPINR_neuro

```{r}

# Select columns we want from SPINR_demo

SPINR_model <- SPINR_demo %>%
  select(1:10) # adjust as needed

# Select the specific columns we want from SPINR_neuro

SPINR_neuro_selected_columns <- SPINR_neuro[, c(1, 2:7)] # we need to also extract record_id to ensure we can join using it. We are also skipping column 2 as thats the group and we don't need a duplicate

# Now we will join the selected column based on the record_id

SPINR_model <- left_join(SPINR_model, SPINR_neuro_selected_columns, by = "record_id")

# Not sure if this is necessary but we are also going to add a new column (age) which will reflect their age at testing calculated using demo_doa and demo_dob -- note that participants dob are all 15-MM-YY to maintain privacy(?)

SPINR_model <- SPINR_model %>%
  mutate(
    demo_dob = as.Date(demo_dob, format = "%Y-%m-%d"), 
    demo_doa = as.Date(demo_doa, format = "%Y-%m-%d"),  
    age = interval(start = demo_dob, end = demo_doa) / years(1)
  )
SPINR_model <- SPINR_model %>%
  select(names(.)[1], age, everything())

```




































# Recruitment 

Load up data

```{r}

Participants <- read.csv("/home/aabdulrasul/Documents/Projects/SPINR/EConsentSPN30Identif_DATA_2024-09-05_1438.csv")

# Run any pre-processing steps here that make the data readable - e.g, turn blanks "" into NA

# Replace blank with NA (see Data Preparation)

Participants <- Participants %>% mutate(across(everything(), ~ replace_blank_with_na(.)))
```
 

We will use the signed form to determine when the participant was recruited into the study. And also whether or not they actually agreed to consenting to the study.

```{r}

## Filter for participants that have consented to the study using the variable consent_agree__1 = 1 (where 1 is "I agree" )

Participants_consented <- Participants %>% filter(consent_agree___1 == 1)

# Now, we will use landing_page_timestamps to determine the date they were recruited into the study

Participants_consented$landing_page_timestamp <- as.Date(Participants_consented$landing_page_timestamp, format = "%Y-%m-%d") # remove the time stamp and ensure the date is properly formatted

```


Summarizing recruitment stats 

```{r}

# Adding a year and month column

Participants_consented <- Participants_consented %>%
  mutate(year = year(landing_page_timestamp), 
         month = floor_date(landing_page_timestamp, "month")) # rounds the month

monthly_recruitment <- Participants_consented %>%
  count(year, month) %>% # counts year and month
  arrange(year, month) %>%
  group_by(year) %>%
  mutate(cumulative_participants = cumsum(n)) # 

# Filter out participants that we may have missed in previous steps

monthly_recruitment <- monthly_recruitment %>%
  filter(!is.na(month), !is.na(n), !is.na(cumulative_participants))


```


### Visualization

Visualization (Accross the years)

```{r}

# Calculate the last cumulative point for each year this will give us the final number of participants recruited that year

last_cumulative <- monthly_recruitment %>%
  group_by(year) %>%
  summarize(last_cumulative = last(cumulative_participants),
            last_month = last(month))

# Plot a grid of a histogram (monthly recrutiment) against a line graph (cumulative recruitment)

ggplot(monthly_recruitment, aes(x = month)) +   # monthly recruitment has all the information we need
  geom_col(aes(y = n), fill = "blue") + # histogtram details
  geom_text(aes(y = n, label = n), vjust = -0.2, color = "black", size = 3) +  # number of participants above hist
  
  geom_line(aes(y = cumulative_participants, group = 1), color = "red", linetype = "dashed") + # line graph of cumulative participants
  geom_text(data = last_cumulative, aes(x = last_month, y = last_cumulative - 5.75, label = last_cumulative), # position adjusted here -8
            vjust = -0.5, hjust = 1.1, color = "red", size = 3) +  # Final cumulative number, adjust size of text here 
  
 # Design plot
  
  labs(title = "Recruitment Overview by Year",
       x = "Month",
       y = "Number of Participants") +
  
  
  # We are going to set x axis free as some years have not been recruiting for the complete year, thereby setting x axis free will give us a clean image that doesn't cut off halfway. We also are including year here to form a grid of plots 
  
  facet_wrap(~year, scales = "free_x") + 

  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # adjust x axis legibility here

```

Visualization (timeline)

```{r}


total_recruitment <- Participants_consented %>% count(month = floor_date(as.Date(Participants_consented$landing_page_timestamp, format = "%Y-%m-%d"), "month")) %>% arrange(month) %>% mutate(cumulative_participants = cumsum(n))


# Plot a grid of a histogram (monthly recrutiment) against a line graph (cumulative recruitment)

ggplot(total_recruitment, aes(x = month)) +   # monthly recruitment has all the information we need
  geom_col(aes(y = n), fill = "blue") + # histogtram details
  geom_text(aes(y = n, label = n), vjust = -0.5, color = "black", size = 3) +  # number of participants above hist
  
 geom_line(aes(y = cumulative_participants, group = 1), color = "red", linetype = "dashed") +  # Dashed line for cumulative participants
  geom_text(data = total_recruitment[nrow(total_recruitment), ], # use total_recruitment to add the final cumulative number of participants recruited into the study 
            aes(x = month, y = cumulative_participants, label = cumulative_participants), # adjust label height, colur and size here 
            hjust = 1.1, vjust = -0.5, color = "red", size = 3)   

  
  labs(title = "Recruitment Overview",
       x = "Month",
       y = "Number of Participants") +
  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # adjust x axis legibility here

```

